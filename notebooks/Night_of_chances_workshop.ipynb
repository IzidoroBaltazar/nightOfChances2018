{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/virtualenvs/noc2/local/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/martin/virtualenvs/noc2/local/lib/python2.7/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import pickle\n",
    "import datetime as dt\n",
    "from time import time\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification, load_iris\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load iris data to pandas dataframe\n",
    "\n",
    "iris = load_iris()\n",
    "temp = iris.target.tolist()\n",
    "names = [iris.target_names[i] for i in temp]\n",
    "\n",
    "iris_df = pandas.DataFrame(data=iris['data'],columns= iris['feature_names'])\n",
    "iris_df['Plant_name'] = names\n",
    "\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the target (label) categories:\n",
    "\n",
    "iris_df['Plant_name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<tr>\n",
    "<td> <img src=\"setosa.jpg\" alt=\"Drawing\" style=\"width: 250px;\"/> </td>\n",
    "<td> <img src=\"versicolor.jpg\" alt=\"Drawing\" style=\"width: 250px;\"/> </td>\n",
    "<td> <img src=\"virginica.jpg\" alt=\"Drawing\" style=\"width: 250px;\"/> </td>\n",
    "</tr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature data types\n",
    "iris_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the dataframe by converting object(string) columns to integer\n",
    "def vectorize_dataframe(df, column_index_str):\n",
    "    \n",
    "    for column_name, column in df.transpose().iterrows():\n",
    "        indexed_col_name = '{}{}'.format(column_index_str, column_name)\n",
    "        if df[column_name].dtype == 'object':\n",
    "            df[indexed_col_name] = df[column_name].astype('category')\n",
    "            df[indexed_col_name] = df[indexed_col_name].cat.codes                \n",
    "        elif is_numeric_dtype(df[column_name]):\n",
    "            df[indexed_col_name] = df[column_name]\n",
    "            \n",
    "    new_df = df.filter(regex='{}*'.format(column_index_str))\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "\n",
    "vect_df = vectorize_dataframe(iris_df, 'indexed_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "print vect_df.dtypes\n",
    "vect_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test and predict\n",
    "\n",
    "def printTable(myDict, colList=None):\n",
    "    if not colList: \n",
    "        colList = list(myDict[0].keys() if myDict else [])\n",
    "    myList = [colList] # 1st row = header\n",
    "    for item in myDict: \n",
    "        myList.append([str(item[col] or '') for col in colList])\n",
    "    colSize = [max(map(len,col)) for col in zip(*myList)]\n",
    "    formatStr = ' | '.join([\"{{:<{}}}\".format(i) for i in colSize])\n",
    "    myList.insert(1, ['-' * i for i in colSize]) # Seperating line\n",
    "    for item in myList: \n",
    "        print(formatStr.format(*item))    \n",
    "    \n",
    "def test_classifiers_on_dataset(classifiers, X_train, y_train, X_test, y_test):\n",
    "\n",
    "    # iterate over classifiers\n",
    "    results=[]\n",
    "    predictions = {}\n",
    "    for clf in classifiers:\n",
    "        \n",
    "        name = str(clf).split('(')[0]\n",
    "        \n",
    "        t0 = time()\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        pred = clf.predict(X_test)\n",
    "        \n",
    "        try:\n",
    "            results.append({'Classifier':name,'Accuracy':round(score, 4),' Time':'{}s'.format(round(time() - t0, 3))})\n",
    "            predictions[name] = pred\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    printTable(results)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset to features & labels(targets) and after that into training and test sets\n",
    "\n",
    "labels = vect_df['indexed_Plant_name']\n",
    "features = vect_df.drop(['indexed_Plant_name'], axis=1)\n",
    "\n",
    "features_train,features_test,labels_train,labels_test = train_test_split(features, labels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier                    |  Time  | Accuracy\n",
      "----------------------------- | ------ | --------\n",
      "KNeighborsClassifier          | 0.003s | 0.9556  \n",
      "SVC                           | 0.003s | 0.9556  \n",
      "DecisionTreeClassifier        | 0.001s | 0.9778  \n",
      "RandomForestClassifier        | 0.025s | 0.9778  \n",
      "AdaBoostClassifier            | 0.022s | 0.9778  \n",
      "GaussianNB                    | 0.001s | 0.9778  \n",
      "QuadraticDiscriminantAnalysis | 0.001s | 1.0     \n",
      "MLPClassifier                 | 0.102s | 1.0     \n"
     ]
    }
   ],
   "source": [
    "classifiers = [\n",
    "        KNeighborsClassifier(3),\n",
    "        SVC(gamma=10, C=10000, kernel=\"rbf\"),\n",
    "        DecisionTreeClassifier(max_depth=5, min_samples_split=3),\n",
    "        RandomForestClassifier(max_depth=4, n_estimators=10, max_features = 4), \n",
    "        AdaBoostClassifier(algorithm='SAMME', random_state=10, learning_rate=0.3, n_estimators=10),\n",
    "        GaussianNB(),\n",
    "        QuadraticDiscriminantAnalysis(),\n",
    "        MLPClassifier(alpha=0.2),\n",
    "        ]\n",
    "\n",
    "predictions = test_classifiers_on_dataset(classifiers, features_train, labels_train, features_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction review:\n",
    "\n",
    "clasifier = 'AdaBoostClassifier'\n",
    "\n",
    "try:\n",
    "    del review_df\n",
    "except:\n",
    "    pass\n",
    "\n",
    "review_df = features_test\n",
    "review_df['Plant_name'] = labels_test\n",
    "review_df['Predictions'] = predictions[clasifier].tolist()\n",
    "# review_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the numbers back to plant names ...\n",
    "\n",
    "def map_values(row, values_dict):\n",
    "    return values_dict[row]\n",
    "\n",
    "values_dict = {0: 'setosa', 1: 'versicolor', 2: 'virginica'};\n",
    "\n",
    "review_df['Plant_name'] = review_df['Plant_name'].apply(map_values, args = (values_dict,))\n",
    "review_df['Predictions'] = review_df['Predictions'].apply(map_values, args = (values_dict,))\n",
    "review_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data visualization #1\n",
    "\n",
    "\n",
    "# Parameters\n",
    "n_classes = 3\n",
    "plot_colors = \"ryb\"\n",
    "plot_step = 0.02\n",
    "\n",
    "# Load data\n",
    "iris = load_iris()\n",
    "\n",
    "for pairidx, pair in enumerate([[0, 1], [0, 2], [0, 3],\n",
    "                                [1, 2], [1, 3], [2, 3]]):\n",
    "    # We only take the two corresponding features\n",
    "    X = iris.data[:, pair]\n",
    "    y = iris.target\n",
    "\n",
    "    # Train\n",
    "    clf = DecisionTreeClassifier().fit(X, y)\n",
    "\n",
    "    # Plot the decision boundary\n",
    "    plt.subplot(2, 3, pairidx + 1)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                         np.arange(y_min, y_max, plot_step))\n",
    "    plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)\n",
    "\n",
    "    plt.xlabel(iris.feature_names[pair[0]])\n",
    "    plt.ylabel(iris.feature_names[pair[1]])\n",
    "\n",
    "    # Plot the training points\n",
    "    for i, color in zip(range(n_classes), plot_colors):\n",
    "        idx = np.where(y == i)\n",
    "        plt.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i],\n",
    "                    cmap=plt.cm.RdYlBu, edgecolor='black', s=15)\n",
    "\n",
    "plt.suptitle(\"Decision surface of a decision tree using paired features\")\n",
    "plt.legend(loc='lower right', borderpad=0, handletextpad=0)\n",
    "plt.axis(\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data visualization #2   (randomly created 2-dimensional dataset)\n",
    "\n",
    "\n",
    "def test_classifiers_on_3_datasets_with_plots():\n",
    "    h = .02  # step size in the mesh\n",
    "\n",
    "    names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "             \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "             \"Naive Bayes\", \"QDA\"]\n",
    "\n",
    "    classifiers = [\n",
    "        KNeighborsClassifier(3),\n",
    "        SVC(kernel=\"linear\", C=0.025),\n",
    "        SVC(gamma=2, C=1),\n",
    "        GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "        DecisionTreeClassifier(max_depth=5),\n",
    "        RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "        MLPClassifier(alpha=1),\n",
    "        AdaBoostClassifier(),\n",
    "        GaussianNB(),\n",
    "        QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "    X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                               random_state=1, n_clusters_per_class=1)\n",
    "    rng = np.random.RandomState(2)\n",
    "    X += 2 * rng.uniform(size=X.shape)\n",
    "    linearly_separable = (X, y)\n",
    "\n",
    "    datasets = [make_moons(noise=0.3, random_state=0),\n",
    "                make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "                linearly_separable\n",
    "                ]\n",
    "\n",
    "    figure = plt.figure(figsize=(27, 9))\n",
    "    i = 1\n",
    "    # iterate over datasets\n",
    "    for ds_cnt, ds in enumerate(datasets):\n",
    "        # preprocess dataset, split into training and test part\n",
    "        X, y = ds\n",
    "        X = StandardScaler().fit_transform(X)\n",
    "        X_train, X_test, y_train, y_test = \\\n",
    "            train_test_split(X, y, test_size=.4, random_state=42)\n",
    "\n",
    "        x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "        y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                             np.arange(y_min, y_max, h))\n",
    "\n",
    "        # just plot the dataset first\n",
    "        cm = plt.cm.RdBu\n",
    "        cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "        if ds_cnt == 0:\n",
    "            ax.set_title(\"Input data\")\n",
    "        # Plot the training points\n",
    "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "                   edgecolors='k')\n",
    "        # and testing points\n",
    "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,\n",
    "                   edgecolors='k')\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        i += 1\n",
    "\n",
    "        # iterate over classifiers\n",
    "        for name, clf in zip(names, classifiers):\n",
    "            ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "            clf.fit(X_train, y_train)\n",
    "            score = clf.score(X_test, y_train)\n",
    "            print 'Dataset: {}, Classifier: {}, Accuracy: {}'.format(ds_cnt, name, score)\n",
    "\n",
    "            # Plot the decision boundary. For that, we will assign a color to each\n",
    "            # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "            if hasattr(clf, \"decision_function\"):\n",
    "                Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "            else:\n",
    "                Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "            # Put the result into a color plot\n",
    "            Z = Z.reshape(xx.shape)\n",
    "            ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "            # Plot also the training points\n",
    "            ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "                       edgecolors='k')\n",
    "            # and testing points\n",
    "            ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
    "                       edgecolors='k', alpha=0.6)\n",
    "\n",
    "            ax.set_xlim(xx.min(), xx.max())\n",
    "            ax.set_ylim(yy.min(), yy.max())\n",
    "            ax.set_xticks(())\n",
    "            ax.set_yticks(())\n",
    "            if ds_cnt == 0:\n",
    "                ax.set_title(name)\n",
    "            ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n",
    "                    size=15, horizontalalignment='right')\n",
    "            i += 1\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "test_classifiers_on_3_datasets_with_plots()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
